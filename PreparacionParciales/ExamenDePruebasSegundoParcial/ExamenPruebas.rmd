---
title: Examen de pruebas
author:
    - Sergio Quijano Rey
    - sergioquijano@correo.ugr.es
date:
output:
    html_document:
        toc: true
        toc_depth: 3
        toc_float: false
        collapsed: true
        number_sections: true
---

# Ejercicio 1

**Enunciado**: Establecer la semilla del generador de números aleatorios escribiendo set.seed(1). A continuación generar 50 valores desde una distribución Chi-cuadrado con n = 30 grados de libertad y almacenarlos en un vector y. Después generar 50 valores desde una distribución Normal con media µ = 30 y desviación típica σ = 5 y almacenarlos en un vector x. A partir de estos vectores se pide resolver la siguientes tareas:

Establecemos la semilla:

```{r}
set.seed(1)
```

Ahora, generamos los 50 valores desde una distribución Chi-cuadrado con 30 grados de libertad:

```{r}
# Parametros del muestreo
df <- 30
n <- 50

# Muestreo
y <- rchisq(n = n, df = df)
y
```

Ahora generamos 50 valores desde una distribución normal de media 30 y desviación típica 5

```{r}
# Parametros del muestreo
n <- 50
mu <- 30
sigma <- 5

# Muestreo
x <- rnorm(n, mean = mu, sd = sigma)
x
```

1. Calcular la media, la desviación típica y los tres cuartiles de y.

```{r}
y.mean <- mean(y)
y.mean

y.sd <- sd(y)
y.sd

y.q1 <- quantile(y, 0.25)
y.q2 <- quantile(y, 0.5)
y.q3 <- quantile(y, 0.75)

y.q1
y.q2
y.q3
```

2. Representar un histograma de los datos almacenados en y.

```{r}
hist(y)
```

3. Superponer al histograma anterior la curva de la densidad Normal con media y desviación típica la de los datos.

```{r}
# Vuelvo a poner el histograma del ej. anterior
# Hago freq = FALSE para usar frecuencia relativa y no absoluta
# Tambien me serviria `hist(y, prob = TRUE)`
hist(y, freq = FALSE)

# Superpongo la curva de la densidad normal
curve(dnorm(x, mean = y.mean, sd = y.sd), col = "darkblue", add = TRUE)
```

4. Representar un gráfico de probabilidad de los valores de y.

```{r}
# TODO -- no estoy nada seguro de esta
hist(y, prob = TRUE)
```

5. Representar un gráfico de cajas de los valores de y.

```{r}
boxplot(y)
```

6. Representar un gráfico de cajas múltiples (2 cajas) que permita comparar la distribución de x e y.

```{r}
boxplot(x, y)
```

7. Representar un diagrama de dispersión de los valores de x (en el eje horizontal) frente a los de y (en el eje vertical).

```{r}
plot(x, y, ylab = "Valores de y", xlab = "Valores de x")
```

8. Ajustar un modelo de regresión lineal $$y_i = \beta_0 + \beta_1 x_i + \epsilon_i ; i \in \Delta_{50}$$ con los datos almacenados en x e y. Representar el modelo ajustado (la recta de regresión) sobre el diagrama de dispersión anterior.

Empiezo realizando el ajuste:

```{r}
# Pongo todo en un dataframe para que sea mas sencillo realizar el ajuste
df <- data.frame(x = x, y = y)

# Aplico el ajuste usando el data frame anterior
mod <- lm(formula = y ~ x, data = df)
mod
```

Ahora superpongo el gráfico anterior sobre el *scatter plot*:

```{r}
# Empiezo haciendo el scatter plot
plot(x, y)

# Ahora añado la linea que me da el ajuste realizado
abline(mod)
```

# Ejercicio 2

**Enunciado**: En 1651, el Caballero de Méré le planteó a Pascal una pregunta relacionada con las apuestas en juegos de azar: ¿sería adecuado apostar a que en cuatro lanzamientos de un dado se obtendrá al menos un seis? Este problema generó una fructífera correspondencia entre Pascal y Fermat que se contribuyó al nacimiento del Cálculo de Probabilidades. Se pide:

1. Escribir una función que simule el lanzamiento de n dados. La función tendrá un único argumento, el número de lanzamientos n, y tomará el valor 4 por defecto. La función devolverá como posibles valores TRUE, si se obtiene al menos un 6 y FALSE en caso contrario

```{r}
# Simula el lanzamiento de `n` dados. Devuelve TRUE si en los `n` lanzamientos
# sale al menos un 6, y devuelve FALSE en otro caso
lanzamiento_dados <- function(n = 4) {
    # Simulamos los lanzamientos de n monedas de seis caras
    monedas <- 1:6
    lanzamientos <- sample(x = monedas, size = n, replace = TRUE)

    # Contamos el numero de lanzamientos en los que sale un 6
    lanzamientos_buenos <- length(which(lanzamientos == 6))

    # Devolvemos si al menos ha habido un lanzamiento bueno
    return(lanzamientos_buenos > 0)
}

lanzamiento_dados()
```

2. Utilizar la función anterior para simular `nsim=10000` jugadas de este juego y calcular la proporción de veces que se gana la apuesta de obtener al menos un 6 en `n = 4` lanzamientos. Comparar el resultado con la probabilidad exacta que es $1 − (\frac{5}{6})^n$.

```{r}
# Establecemos los parametros de la simulacion
simulaciones <- 1000
n <- 4

# Realizamos las simulaciones
resultados <- vector()
for(i in 1:simulaciones){
    resultados[i] <- lanzamiento_dados(n)
}

# Calculamos las simulaciones con exito y sin exito
exito <- sum(resultados)
fracaso <- sum(!resultados)

# Calculamos ahora la probabilidad
prob <- exito / (exito + fracaso)
prob

# Comparamos ese valor con el real
prob_real <- 1 - (5/6)^n
prob_real

# Realizamos una comparacion mas numerica
abs_err <- abs(prob - prob_real)
abs_err

perc_err <- abs_err / prob_real * 100
perc_err
```

# Ejercicio 3

**Enunciado**: La distribución Pareto de parámetros a, b > 0 corresponde a una función inversa de probabilidad: $$x = F^{-1}(u) = \frac{b}{(1 - u)^{\frac{1}{a}}}$$ Utilizando el método de inversión se pide generar n = 1000 valores de un distribución Pareto de parámetros a = 5 y b = 4. Evaluar usando gráficos y el contraste de Kolmogorov-Smirknow que en efecto los valores generados provienen de dicha distribución.

Vamos a usar una función que resuelve la tarea en general:

```{r}
# Funcion que devuelve el
# `n` numero de valores que queremos generar de la distribucion
# `F_inv` función de densidad inversa
metodo_inversion <- function(n, F_inv) {

    # Generamos n valores de una uniforme 0, 1
    U <- runif(n = n, min = 0, max = 1)

    # Usamos la función inversa para generar el muestreo simulado
    X <- F_inv(U)

    # Devolvemos dichos valores
    return(X)
}
```

Con esto, pasamos a generar los 1000 valores de dicha distribución:

```{r}
# Parametros de la simulacion
a <- 5
b <- 4
n <- 1000
F_inv <- function(u) (b) / ((1 - u)^(1 / a))

# Tomamos los puntos
x <- metodo_inversion(n, F_inv)
```

Mostramos los puntos obtenidos en un histograma:

```{r}
hist(x)
```

Veamos ahora que los datos se asimilan a tomar la distribución como tal. Empezamos comparando la función de densidad que nos da el histograma con la función de densidad real:

```{r}
# Empezamos con el histograma
hist(x, prob = TRUE)

# Añadimos las lineas suavizadas que vienen del histograma
lines(density(x), col = "blue")

# Ahora añadimos la linea de densidad real de la distribucion
f <- function(x) (a*b^a) / (x^(a+1))
curve(f(x), add = TRUE, col = "red")
```

Gráficamente podemos ver que la aproximación conseguida es muy buena. Veamos esto con un contraste de Kolmogorov-Smirknow:

```{r}
# Definimos la funcion de distribucion, que tenemos que usar para el ks.test
# La funcion tiene que aceptar los valores de a, b para
F <- function(x, a, b) (1 - (b / x)^a) * (x > b)

# Aplicamos el test
ks.test(x, F, a = a, b = b)
```

# Ejercicio 4

**Enunciado**: Utilizando integración de Monte Carlo se pide aproximar la integral $$\int_0^1 \frac{1}{1 + x^2} dx$$ y calcular el error de la aproximación. Considerar para ello un número sucientemente grande de simulaciones, evaluando la convergencia mediante un gráfico. Como referencia tener en cuenta que el valor exacto de la integral es $\frac{\pi}{4}$.

Definimos la función a integrar:

```{r}
f <- function(x) 1 / (1 + x * x)
```

Con esto, realizamos la **integración Montecarlo clásica**. Empezamos expresando esto en una función, que puede ser re-usada:

```{r}
# Funcion para realizar la aproximacion de una integral en el intervalo 0, 1
#
# PARAMETERS
# ==========
# `nsim` numero de simulaciones que queremos lanzar
# `f` funcion que estamos integrando en (0, 1)
# `seed` semilla aleatoria que queremos usar
# `show_plot` TRUE si queremos que se muestre el grafico de aproximacion
#
# RETURNS
# =======
# `approx_int` valor de la aproximacion
# `error` estimacion del error cometido
# `approx_seq` secuencia de valores de aproximaciones a la integral
montecarlo_simple <- function(nsim = 1000, f, seed = 1, show_plot = FALSE) {
    # Fijamos la semilla aleatoria
    set.seed(1)

    # Muestreamos de una uniforme 0, 1 y aplicamos la funcion sobre esa muestra
    x <- runif(n = nsim, min = 0, max = 1)
    fx <- f(x)

    # Realizamos la aproximacion usando los valores previamente calculados
    # En vez de guardar el valor final de la aproximacion, guardamos la secuencia de aproximaciones
    # para poder mostrar graficos de convergencai
    approx_int_seq <- cumsum(fx) / (1:nsim)

    # Calculamos los errores en la aproximacion
    error <- sqrt(cumsum((fx - approx_int_seq)^2)) / (1:nsim)

    # Mostramos graficamente los errores si asi se ha dado por parametro
    if(show_plot == TRUE) {
        plot(
            1:nsim,
            approx_int_seq,
            type = "l",
            ylab = "Aproximación y límites de error",
            xlab = "Número de simulaciones"
        )
        z <- qnorm(0.025, lower.tail = FALSE)
        lines(approx_int_seq - z * error, col = "blue", lwd = 2, lty = 3)
        lines(approx_int_seq + z * error, col ="blue", lwd = 2, lty = 3)
        abline(h = approx_int_seq[nsim], col = 2)
    }

    # Devolvemos los datos, esto es, la aproximacion final, el error de la
    # aproximacion y toda la secuencia
    return(list(approx_int = approx_int_seq[nsim], error = error[nsim], approx_seq = approx_int_seq))
}
```

Nos aprovechamos de esta función para aproximar la integral, viendo el gráfico de aproximación y el error cometido:

```{r}
# Fijamos los parametros de la simulacion
nsim <- 1000

res <- montecarlo_simple(nsim = nsim, f = f, seed = 1, show_plot = TRUE)
res$approx_int
res$error
```

Ahora podemos comparar con el valor real de la integral, que es $\frac{\pi}{4}$:

```{r}
real_int <- pi / 4
abs_err <- abs(res$approx_int - real_int)
abs_err

rel_err <- abs_err / real_int * 100
rel_err
```

El error relativo es aproximadamente $0.08\%$, que es un valor completamente satisfactorio para nuestros objetivos.
